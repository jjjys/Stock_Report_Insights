{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade google-ai-generativelanguage\n",
        "!pip install -q -U google-generativeai\n",
        "!pip install ollama pydantic pdfplumber langchain\n",
        "\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASVedBBJHA4h",
        "outputId": "da110751-d0bd-4370-9a5c-2c67569b9390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ollama in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.10)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.12/dist-packages (0.11.7)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (5.0.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.37)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 실행 환경\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# 모델\n",
        "from google import genai\n",
        "import ollama\n",
        "from ollama import chat\n",
        "\n",
        "# RAG\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from pydantic import BaseModel\n",
        "import pdfplumber\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datetime import datetime\n",
        "import os, subprocess, time, json, re\n",
        "import warnings\n",
        "from typing import Callable\n",
        "\n",
        "# 병렬처리\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed#, ProcessPoolExecutor"
      ],
      "metadata": {
        "id": "el1lwY2QG5zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "코랩 환경 여부"
      ],
      "metadata": {
        "id": "ivHXUegwIyUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab  # 모듈이 있다면 Colab 환경\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "IS_COLAB_ENV = is_colab()"
      ],
      "metadata": {
        "id": "pD4jzrbMJwOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 전처리"
      ],
      "metadata": {
        "id": "2xOMpPqrd8WH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF 임포트"
      ],
      "metadata": {
        "id": "HQkrQqBWdKas"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52f69676",
        "outputId": "239b34d2-3e84-4485-c208-4d3296343367"
      },
      "source": [
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4438d683",
        "outputId": "5103373a-b6ff-4f27-aecd-3092dc9bc104"
      },
      "source": [
        "pdf_path = '/content/drive/My Drive/이어드림/pdfs'\n",
        "\n",
        "if os.path.exists(pdf_path):\n",
        "    print(f\"디렉터리 '{pdf_path}'가 존재합니다.\")\n",
        "    # print(\"디렉터리 내용:\")\n",
        "    # for item in os.listdir(pdf_path):\n",
        "    #     print(item)\n",
        "else:\n",
        "    print(f\"디렉터리 '{pdf_path}'가 존재하지 않습니다.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "디렉터리 '/content/drive/My Drive/이어드림/pdfs'가 존재합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 5개만\n",
        "# 실제 실행 시는 이거 주석처리\n",
        "pdf_path = '/content/drive/My Drive/이어드림/pdfs_test'"
      ],
      "metadata": {
        "id": "Kyr7hs_3goCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cafe8589"
      },
      "source": [
        "def get_report_pdf_files(directory_path, is_test:bool=False, test_num:int=5, verbose:bool=False) -> list:\n",
        "    all_items = os.listdir(directory_path)\n",
        "\n",
        "    # PDF 필터링\n",
        "    pdf_files = [item for item in all_items if item.endswith('.pdf')]\n",
        "\n",
        "    if is_test:\n",
        "        pdf_files = pdf_files[:test_num]\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Selected PDF files for processing:\")\n",
        "      for file in pdf_files:\n",
        "          print(file)\n",
        "\n",
        "    return pdf_files\n",
        "\n",
        "\n",
        "# selected_pdf_files = get_report_pdf_files(pdf_path, is_test=True, test_num=5)\n",
        "# selected_pdf_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a highly skilled information extraction bot.\n",
        "Your task is to extract specific information from the provided securities report PDF file.\n",
        "Extract the following details and return them in JSON format:\n",
        "\n",
        "- 종목명 (Stock Name)\n",
        "- 종목코드 (티커) (Stock Code/Ticker)\n",
        "- 작성일 (Date of Report)\n",
        "- 현재 주가 (Current Stock Price - only numeric value)\n",
        "- 목표 주가 (Target Stock Price - only numeric value)\n",
        "- 투자 의견 (Investment Opinion - only in \"Buy\", \"Hold\" or \"Sell\")\n",
        "- 작성 애널리스트 (Author Analyst)\n",
        "- 소속 증권사 (Affiliated Securities Firm)\n",
        "\n",
        "If a piece of information is not found, use 'N/A' for string values and 0 for numeric values.\n",
        "\n",
        "Return only the JSON object. Do not include any other text.\n",
        "\n",
        "Example JSON format:\n",
        "{{\n",
        "  \"종목명\": \"Example Stock\",\n",
        "  \"종목코드\": \"000000\",\n",
        "  \"작성일\": \"YYYY-MM-DD\",\n",
        "  \"현재 주가\": 10000,\n",
        "  \"목표 주가\": 12000,\n",
        "  \"투자 의견\": \"BUY\",\n",
        "  \"작성 애널리스트\": \"Analyst Name\",\n",
        "  \"소속 증권사\": \"Securities Firm Name\"\n",
        "}}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5J0PjH0Otn-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gemini api"
      ],
      "metadata": {
        "id": "_KCl8QHQlkNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_gemini(directory_path:str, file_name:str, prompt:str=system_prompt, api_key:str=None, return_dict:bool=True) -> str:\n",
        "    if api_key is None:\n",
        "        if IS_COLAB_ENV:\n",
        "            api_key = userdata.get('GOOGLE_API_KEY') # Load the API key from Colab secrets\n",
        "        else:\n",
        "            load_dotenv()\n",
        "            api_key = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "    client = genai.Client(api_key=api_key)\n",
        "\n",
        "    if file_name is not None:\n",
        "        file_path = os.path.join(directory_path, file_name)\n",
        "\n",
        "        try:\n",
        "            # Upload the file using the genai client\n",
        "            sample_file = client.files.upload(file=file_path)\n",
        "\n",
        "            # Generate content using the uploaded file and the prompt\n",
        "            response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
        "                                                      contents=[sample_file, prompt])\n",
        "\n",
        "            result = response.text.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "\n",
        "            if return_dict:\n",
        "                return json.loads(result)\n",
        "            else:\n",
        "                return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "    else:\n",
        "        print(\"No PDF files were selected for processing. Please run the previous cell.\")"
      ],
      "metadata": {
        "id": "GE7zdR3gUszD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트"
      ],
      "metadata": {
        "id": "Pe1OrGXLlTN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for pdf_file in selected_pdf_files:\n",
        "#     print(\"Extracted Information (JSON) =>\", pdf_file)\n",
        "#     print(ask_gemini(pdf_path, pdf_file))"
      ],
      "metadata": {
        "id": "KGi7YByUlAW1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ollama 로컬 (qwen, llama)"
      ],
      "metadata": {
        "id": "KwksNU95Gjbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ollama 사용하지 않을 경우 실행 셀 주석처리**"
      ],
      "metadata": {
        "id": "gFNl-vML_ywM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ollama 실행"
      ],
      "metadata": {
        "id": "s7hgxe-4JOi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "# time.sleep(1)"
      ],
      "metadata": {
        "id": "5355axNOpn2g",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 로컬 모델 저장"
      ],
      "metadata": {
        "id": "3nn11NyLJQ4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ollama_model_check(drive_model_dir:str=None, require_lite:bool=True) -> str:\n",
        "    drive_model_dir = \"/content/drive/MyDrive/ollama_models\" if drive_model_dir is None else drive_model_dir\n",
        "    local_model_dir = os.path.expanduser(\"~/.ollama\")\n",
        "\n",
        "    # 모델 저장 폴더 설정\n",
        "    os.makedirs(drive_model_dir, exist_ok=True)\n",
        "    # os.makedirs(os.path.expanduser(\"~/.ollama\"), exist_ok=True)\n",
        "\n",
        "    if os.path.exists(local_model_dir):\n",
        "        os.system(f\"rm -rf {local_model_dir}\")\n",
        "    os.system(f\"ln -s {drive_model_dir} {local_model_dir}\")\n",
        "\n",
        "    model_names = [\"qwen3:8b\", \"llama3:8b\"] if not require_lite else [\"qwen3:1.7b\", \"llama3:8b\"]\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True, check=True)\n",
        "        installed_models_output = result.stdout\n",
        "    except subprocess.CalledProcessError:\n",
        "        installed_models_output = \"\"\n",
        "\n",
        "    for model in model_names:\n",
        "        if model in installed_models_output:\n",
        "            print(f\"{model} 이미 설치됨 — 다운로드 생략\")\n",
        "        else:\n",
        "            print(f\"{model} 다운로드 중...\")\n",
        "            subprocess.run([\"ollama\", \"pull\", model], check=True)\n",
        "\n",
        "    print(\"\\n모델 확인\")\n",
        "\n",
        "    return drive_model_dir"
      ],
      "metadata": {
        "id": "HnRayeXVpwsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # %time\n",
        "# model_path = ollama_model_check()"
      ],
      "metadata": {
        "id": "cCS1fr_spykw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "로컬 환경 ollama 모델 리스트"
      ],
      "metadata": {
        "id": "fsLfi-PeJXOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !ollama list"
      ],
      "metadata": {
        "id": "9YBkiChEp1G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 출력 포맷"
      ],
      "metadata": {
        "id": "esVlug6hJb72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReportInfo(BaseModel):\n",
        "    stock: str\n",
        "    ticker: str\n",
        "    published_date: str\n",
        "    current_price: float\n",
        "    target_price: float\n",
        "    investment_opinion: bool\n",
        "    author_analyst: str\n",
        "    affiliated_firm: str"
      ],
      "metadata": {
        "id": "BgVXkQGGadBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ollama_prompts = {\"종목명\": \"본 보고서가 취급하는 종목명이 무엇인가요?\",\n",
        "                  \"종목코드\": \"본 보고서가 취급하는 기업의 티커(ticker)가 무엇인가요?\",\n",
        "                  \"작성일\": \"본 보고서가 발행된 일시를 yyyy-mm-dd 형태로 답하세요.\",\n",
        "                  \"현재 주가\": \"본 보고서에 발표된 현재 주가를 답하세요. (KRW)\",\n",
        "                  \"목표 주가\": \"본 보고서에 발표된 목표 주가를 답하세요. (KRW)\",\n",
        "                  \"투자 의견\": \"본 보고서에 발표된 투자 의견을 답하세요. (True: 매수, False: 보유)\",\n",
        "                  \"작성 애널리스트\": \"본 보고서에 발표된 작성 애널리스트는 누구인가요?\",\n",
        "                  \"소속 증권사\": \"본 보고서에 발표된 소속 증권사의 기업명은 무엇인가요?\"}"
      ],
      "metadata": {
        "id": "FTYE3M4IrNyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문서 참조"
      ],
      "metadata": {
        "id": "PDJ2r5qJJx2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF에서 표, 그래프 제외한 텍스트만 추출"
      ],
      "metadata": {
        "id": "lPwRwFObJ31m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_to_text(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# def pdf_to_text_no_tables(pdf_path):\n",
        "#     text = \"\"\n",
        "#     with pdfplumber.open(pdf_path) as pdf:\n",
        "#         for page in pdf.pages:\n",
        "#             # 페이지의 표 영역 추출\n",
        "#             tables = page.find_tables()\n",
        "#             table_bboxes = [table.bbox for table in tables]  # bbox = (x0, top, x1, bottom)\n",
        "\n",
        "#             # 문자 단위로 필터링\n",
        "#             page_text = \"\"\n",
        "#             for char in page.chars:\n",
        "#                 in_table = any(\n",
        "#                     bbox[0] <= char['x0'] <= bbox[2] and\n",
        "#                     bbox[1] <= char['top'] <= bbox[3]\n",
        "#                     for bbox in table_bboxes\n",
        "#                 )\n",
        "#                 if not in_table:\n",
        "#                     page_text += char['text']\n",
        "#             text += page_text + \"\\n\"\n",
        "#     return text"
      ],
      "metadata": {
        "id": "UJfIgWdFhTNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "코사인 유사도"
      ],
      "metadata": {
        "id": "1hIOIumpJ8OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_simliar_chunks(store, query_vector, chunks:list, k:int=2):\n",
        "    # Convert the store (list of vectors) to a numpy array\n",
        "    store_np = np.array(store)\n",
        "    query_vector_np = np.array(query_vector)\n",
        "\n",
        "    # Calculate cosine similarity between the query vector and each chunk vector\n",
        "    dot_products = np.dot(store_np, query_vector_np)\n",
        "    norm_store = np.linalg.norm(store_np, axis=1)\n",
        "    norm_query = np.linalg.norm(query_vector_np)\n",
        "\n",
        "    # Avoid division by zero for zero vectors\n",
        "    norm_store[norm_store == 0] = 1e-10\n",
        "    norm_query = norm_query if norm_query != 0 else 1e-10\n",
        "\n",
        "    cosine_similarities = dot_products / (norm_store * norm_query)\n",
        "\n",
        "    # Get the indices that would sort the cosine similarities in descending order\n",
        "    sorted_indices = np.argsort(cosine_similarities)[-1:-k-1:-1]\n",
        "\n",
        "    # Get the sorted chunks and similarity scores\n",
        "    sorted_chunks = [chunks[i] for i in sorted_indices]\n",
        "    sorted_scores = cosine_similarities[sorted_indices]\n",
        "\n",
        "    # Return the sorted chunks and similarity scores\n",
        "    return sorted_chunks, sorted_scores"
      ],
      "metadata": {
        "id": "T4WWTxtvhjlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_ollama(directory_path:str, file_name:str, model_name:str, prompt:str=system_prompt, format:BaseModel=ReportInfo, return_dict:bool=True):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "\n",
        "    file_path = os.path.join(directory_path, file_name)\n",
        "    text = pdf_to_text(file_path)\n",
        "    chunks = text_splitter.split_text(text)\n",
        "\n",
        "    store_emb = ollama.embed(model=model_name, input=chunks)['embeddings'] # Extract embeddings from the EmbedResponse objects\n",
        "    topk_chunks = []\n",
        "\n",
        "    for question in ollama_prompts.values():\n",
        "        query_v_emb = ollama.embed(model=model_name, input=question)['embeddings'][0]\n",
        "        rel_chunks, _ = get_simliar_chunks(store_emb, query_v_emb, chunks)\n",
        "        topk_chunks.extend(rel_chunks)\n",
        "        # print(question, \"\\n==>\", rel_chunks)\n",
        "\n",
        "    response = chat(model=model_name,\n",
        "                    messages=[{'role': 'system', 'content': prompt},\n",
        "                              {'role': 'user', 'content': 'Give me the context!'},\n",
        "                              {'role': 'assistant', 'content': '\\n'.join(topk_chunks)},\n",
        "                              {'role': 'user', 'content': 'Return a JSON object from the reference as per my instructions.'}],\n",
        "                    format=ReportInfo.model_json_schema(),\n",
        "                    # think=True,\n",
        "                    stream=False)\n",
        "\n",
        "    result = ReportInfo.model_validate_json(response.message.content)\n",
        "\n",
        "    if return_dict:\n",
        "        return result.model_dump()\n",
        "    else:\n",
        "        return result.model_dump_json(indent=2)"
      ],
      "metadata": {
        "id": "ml4rOI9uhoDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트"
      ],
      "metadata": {
        "id": "RtRAxanf_mhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ask_ollama('/content/drive/My Drive/이어드림/pdfs', \"210216_4분기는_성수기_1분기는_최성수기.pdf\", \"llama3:8b\")"
      ],
      "metadata": {
        "id": "_tt_c-zGgvgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for pdf_file in ['210226_이제_조정은_충분하다.pdf', '210226_플랫폼_기반의_간편건강식_전문기업_중국_시.pdf', '210226_재무_건전성_및_투자여력_확보.pdf',\n",
        "#                   '210302_DRAM_가격_전망치_상향_조정.pdf', '210226_소방설비_제작부터_소방시설_공사까지_종합_.pdf']:\n",
        "#     print(\"Extracted Information (JSON) =>\", pdf_file)\n",
        "#     print(ask_ollama(pdf_path, pdf_file, \"llama3:8b\"))"
      ],
      "metadata": {
        "id": "Bs7YHPrrjL67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 목표 주가 수집"
      ],
      "metadata": {
        "id": "RTqlylfvd6cA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pykrx\n",
        "\n",
        "from pykrx import stock\n",
        "import psycopg2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DYGrhMteQa2",
        "outputId": "7e83178b-684a-47a1-8b4b-e88a2e01c68e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pykrx in /usr/local/lib/python3.12/dist-packages (1.0.51)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pykrx) (2.32.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from pykrx) (2.2.2)\n",
            "Requirement already satisfied: datetime in /usr/local/lib/python3.12/dist-packages (from pykrx) (5.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pykrx) (2.0.2)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.12/dist-packages (from pykrx) (2.0.2)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.12/dist-packages (from pykrx) (1.2.18)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.12/dist-packages (from pykrx) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from pykrx) (3.10.0)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.12/dist-packages (from datetime->pykrx) (8.0.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from datetime->pykrx) (2025.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated->pykrx) (1.17.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pykrx) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pykrx) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pykrx) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pykrx) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pykrx) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pykrx) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pykrx) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pykrx) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->pykrx) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pykrx) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pykrx) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pykrx) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pykrx) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->pykrx) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_target_hit_date(ticker:str, report_date:str, target_price:float):\n",
        "    start_date = report_date.replace(\"-\", \"\")\n",
        "    end_date = datetime.today().strftime(\"%Y%m%d\")\n",
        "\n",
        "    df = stock.get_market_ohlcv_by_date(start_date, end_date, ticker)\n",
        "    df = df[[\"종가\"]]\n",
        "\n",
        "    reached = df[df[\"종가\"] >= target_price]\n",
        "\n",
        "    if not reached.empty:\n",
        "        first_hit_date = reached.index[0].strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        report_dt = datetime.strptime(report_date, \"%Y-%m-%d\")\n",
        "        hit_dt = datetime.strptime(first_hit_date, \"%Y-%m-%d\")\n",
        "\n",
        "        return first_hit_date, (hit_dt - report_dt).days\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "X5rC_PPhtNxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF 전처리 파이프라인"
      ],
      "metadata": {
        "id": "sJ2UGCUy7tWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF로부터 LLM이 추출한 데이터 중 필수 데이터 검증"
      ],
      "metadata": {
        "id": "GnEkltZ2hK9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_validate_report_data(report_data:dict) -> bool:\n",
        "    \"\"\"\n",
        "    종목코드, 작성일, 목표 주가 => 필수\n",
        "    종목명, 현재 주가, 투자 의견, 작성 애널리스트, 소속 증권사 => 선택\n",
        "    \"\"\"\n",
        "    na_val = [None, \"N/A\", \"n/a\", \"\", 0]\n",
        "    return report_data \\\n",
        "          and isinstance(report_data, dict) \\\n",
        "          and report_data.get(\"종목코드\") not in na_val \\\n",
        "          and report_data.get(\"작성일\") not in na_val \\\n",
        "          and report_data.get(\"목표 주가\") not in na_val"
      ],
      "metadata": {
        "id": "kJTbzoq4W_WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "단일 PDF 파일"
      ],
      "metadata": {
        "id": "sAsC7YTV705r"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd8d55ed"
      },
      "source": [
        "def process_single_pdf(file_name:str, directory_path:str, llm_fn:Callable, llm_fn_kwargs:dict=None) -> dict:\n",
        "    \"\"\"\n",
        "    Processes a single PDF file by extracting info and finding target hit date.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        llm_fn_kwargs = {} if llm_fn_kwargs is None else llm_fn_kwargs\n",
        "        # Task 1: Extract information using ask_gemini (I/O-bound)\n",
        "        report_info = llm_fn(directory_path, file_name, **llm_fn_kwargs)\n",
        "\n",
        "        # Check if extraction was successful and essential info is available\n",
        "        if is_validate_report_data(report_info):\n",
        "            ticker = report_info[\"종목코드\"]\n",
        "            report_date = report_info[\"작성일\"]\n",
        "            target_price = report_info[\"목표 주가\"]\n",
        "\n",
        "            # Task 2: Find target hit date (potentially CPU-bound, but often quick with PyKRX)\n",
        "            hit_date_info = find_target_hit_date(ticker, report_date, target_price)\n",
        "\n",
        "            # Combine the results\n",
        "            return {\n",
        "                \"pdf_file\": file_name,\n",
        "                \"report_info\": report_info,\n",
        "                \"hit_date_info\": hit_date_info\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"pdf_file\": file_name,\n",
        "                \"report_info\": report_info,\n",
        "                \"hit_date_info\": \"Could not extract essential information for target hit date.\"\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_name}: {e}\")\n",
        "        return {\n",
        "            \"pdf_file\": file_name,\n",
        "            \"report_info\": None,\n",
        "            \"hit_date_info\": f\"Error: {e}\"\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이프라인 (DB 작업 분리)"
      ],
      "metadata": {
        "id": "DzK1J30Q72R8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def report_preprocessing_parallel(directory_path:str, get_files_fn:Callable, pipeline_fn:Callable, llm_fn:Callable, llm_fn_kwargs:dict=None, num_workers:int=5, verbose:bool=True) -> list:\n",
        "    \"\"\"\n",
        "    Parallel Processing Implementation for Report Preprocessing.\n",
        "    \"\"\"\n",
        "    processed_results = []\n",
        "\n",
        "    # Using ThreadPoolExecutor for parallel execution\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        # Get the list of PDF files using the executor\n",
        "        future_to_get_files = executor.submit(get_files_fn, directory_path)\n",
        "        selected_pdf_files = future_to_get_files.result() # Wait for the file list to be ready\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Starting parallel processing for {len(selected_pdf_files)} files...\")\n",
        "\n",
        "        # Submit tasks for processing each PDF file\n",
        "        future_to_pdf = {executor.submit(pipeline_fn, pdf_file, directory_path, llm_fn, llm_fn_kwargs): pdf_file for pdf_file in selected_pdf_files}\n",
        "\n",
        "        # Process the results as they complete\n",
        "        for future in as_completed(future_to_pdf):\n",
        "            pdf_file = future_to_pdf[future]\n",
        "            try:\n",
        "                result = future.result() # result like dict {\"pdf_file\": str, \"report_info\": json, \"hit_date_info\": tuple}\n",
        "                processed_results.append(result)\n",
        "                if verbose:\n",
        "                    print(f\"Successfully processed data for: {pdf_file}\")\n",
        "            except Exception as exc:\n",
        "                print(f'{pdf_file} generated an exception during processing: {exc}')\n",
        "\n",
        "    return processed_results"
      ],
      "metadata": {
        "id": "cC1s-A7D3MtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 다중 LLM 함수 사용 시에도 파라미터 등 달라서 아래 함수 다중 호출로 처리\n",
        "# processed_results = report_preprocessing_parallel(pdf_path, get_report_pdf_files, process_single_pdf, ask_gemini)\n",
        "\n",
        "# # --- Displaying Results ---\n",
        "# print(\"\\n--- Summary of Processed Results ---\")\n",
        "# for result in processed_results:\n",
        "#     print(f\"File: {result['pdf_file']}\")\n",
        "#     print(f\"  Report Info: {result['report_info']}\")\n",
        "#     print(f\"  Target Hit Info: {result['hit_date_info']}\")\n",
        "#     print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "5zG5Ou6r8J_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이프라인 (DB 작업 통합)"
      ],
      "metadata": {
        "id": "bmhrmGAvfO6B"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6bf30e2"
      },
      "source": [
        "def report_preprocessing_parallel_with_db(directory_path:str, get_files_fn:Callable, pipeline_fn:Callable, llm_fn:Callable, llm_fn_kwargs:dict=None,\n",
        "                                          num_workers:int=5, verbose:bool=True, conn=None, cursor=None) -> tuple:\n",
        "    \"\"\"\n",
        "    Parallel Processing Implementation for Report Preprocessing with direct DB insertion.\n",
        "    Assumes DB connection 'conn' and cursor 'cursor' are available in the scope where this function is called.\n",
        "    \"\"\"\n",
        "    if conn is None or cursor is None:\n",
        "        raise ValueError(\"DB connection and cursor must be provided.\")\n",
        "\n",
        "    processed_results = []\n",
        "\n",
        "    # Using ThreadPoolExecutor for parallel execution\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        # Get the list of PDF files using the executor\n",
        "        future_to_get_files = executor.submit(get_files_fn, directory_path)\n",
        "        selected_pdf_files = future_to_get_files.result() # Wait for the file list to be ready\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Starting parallel processing for {len(selected_pdf_files)} files...\")\n",
        "\n",
        "        # Submit tasks for processing each PDF file\n",
        "        future_to_pdf = {executor.submit(pipeline_fn, pdf_file, directory_path, llm_fn, llm_fn_kwargs): pdf_file for pdf_file in selected_pdf_files}\n",
        "\n",
        "        # Process the results as they complete\n",
        "        for future in as_completed(future_to_pdf):\n",
        "            pdf_file = future_to_pdf[future]\n",
        "            try:\n",
        "                result = future.result() # result like dict {\"pdf_file\": str, \"report_info\": json, \"hit_date_info\": tuple}\n",
        "                processed_results.append(result)\n",
        "\n",
        "                # --- Direct DB Insertion within the loop ---\n",
        "                report_info = result.get(\"report_info\")\n",
        "                hit_date_info = result.get(\"hit_date_info\")\n",
        "                file_name = result.get(\"pdf_file\")\n",
        "\n",
        "                # Check if essential information was extracted successfully before attempting insert\n",
        "                if is_validate_report_data(report_info):\n",
        "                    # Prepare data for report_info table\n",
        "                    # If a constraint error occurs, judge as a data error and passed\n",
        "                    report_info_data = (\n",
        "                        file_name,\n",
        "                        report_info.get(\"종목명\"),\n",
        "                        report_info.get(\"종목코드\"),\n",
        "                        report_info.get(\"작성일\"),\n",
        "                        report_info.get(\"현재 주가\"),\n",
        "                        report_info.get(\"목표 주가\"),\n",
        "                        report_info.get(\"투자 의견\").lower() == \"buy\",\n",
        "                        report_info.get(\"작성 애널리스트\"),\n",
        "                        report_info.get(\"소속 증권사\")\n",
        "                    )\n",
        "\n",
        "                    # Prepare data for report_hit table\n",
        "                    hit_date = hit_date_info[0] if isinstance(hit_date_info, tuple) else None\n",
        "                    hit_days = hit_date_info[1] if isinstance(hit_date_info, tuple) else None\n",
        "\n",
        "                    # If both hit_date and hit_days are None, judge as a Hit miss\n",
        "                    report_hit_data = (\n",
        "                        file_name,\n",
        "                        hit_date,\n",
        "                        hit_days\n",
        "                    )\n",
        "\n",
        "                    try:\n",
        "                        cursor.execute(\"\"\"\n",
        "                            INSERT INTO report_info (pdf_file, stock, ticker, published_date, current_price, target_price, investment_opinion, author_analyst, affiliated_firm)\n",
        "                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);\n",
        "                        \"\"\", report_info_data)\n",
        "\n",
        "                        cursor.execute(\"\"\"\n",
        "                            INSERT INTO report_hit (pdf_file, hit_date, hit_days)\n",
        "                            VALUES (%s, %s, %s);\n",
        "                        \"\"\", report_hit_data)\n",
        "\n",
        "                        conn.commit()\n",
        "                        if verbose:\n",
        "                            print(f\"Successfully processed and inserted data for: {pdf_file}\")\n",
        "\n",
        "                    except (Exception, psycopg2.Error) as db_error:\n",
        "                        conn.rollback()\n",
        "                        print(f\"Database INSERT error for {pdf_file}: {db_error}\")\n",
        "                        # Log this error or handle it as needed without stopping the loop\n",
        "\n",
        "                else:\n",
        "                    if verbose:\n",
        "                         print(f\"Skipping DB insert for {pdf_file}: Essential info missing or processing error.\")\n",
        "\n",
        "\n",
        "            except Exception as exc:\n",
        "                print(f'{pdf_file} generated an exception during processing: {exc}')\n",
        "                # This catches errors during the PDF processing pipeline_fn\n",
        "\n",
        "    return processed_results, conn, cursor # You might still want to return results for logging or further processing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DB 연결"
      ],
      "metadata": {
        "id": "PXB0IPZUh5ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if IS_COLAB_ENV:\n",
        "    postgres_key = userdata.get('POSTGRES_KEY')\n",
        "else:\n",
        "    load_dotenv()\n",
        "    postgres_key = os.getenv('POSTGRES_KEY')\n",
        "\n",
        "db_name = \"stockdb\"\n",
        "db_user = \"stock\"\n",
        "\n",
        "user_config_query = f\"CREATE USER {db_user} WITH PASSWORD '{postgres_key}';\"\n",
        "db_config_query = f\"CREATE DATABASE {db_name} OWNER {db_user};\""
      ],
      "metadata": {
        "id": "m6B6h99fjKhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo apt-get update\n",
        "!sudo apt-get install -y postgresql\n",
        "\n",
        "!sudo service postgresql start\n",
        "\n",
        "# Drop user and database if they exist to ensure a clean setup\n",
        "!sudo -u postgres psql -c \"DROP DATABASE IF EXISTS {db_name};\"\n",
        "!sudo -u postgres psql -c \"DROP USER IF EXISTS {db_user};\"\n",
        "\n",
        "# Create the user with password and login privileges\n",
        "!sudo -u postgres psql -c \"CREATE USER {db_user} WITH PASSWORD '{postgres_key}' LOGIN;\"\n",
        "\n",
        "# Create the database owned by the user\n",
        "!sudo -u postgres psql -c \"CREATE DATABASE {db_name} OWNER {db_user};\"\n",
        "\n",
        "!pip install psycopg2-binary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkkYvgBChY-5",
        "outputId": "35d6bb9a-339f-4b7b-efd0-5474449febd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "postgresql is already the newest version (14+238).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n",
            "DROP DATABASE\n",
            "DROP ROLE\n",
            "CREATE ROLE\n",
            "CREATE DATABASE\n",
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.12/dist-packages (2.9.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "\n",
        "# 연결 정보 설정\n",
        "try:\n",
        "    # 데이터베이스 연결\n",
        "    conn = psycopg2.connect(\n",
        "        host=\"localhost\",\n",
        "        dbname=db_name,\n",
        "        user=db_user,\n",
        "        password=postgres_key\n",
        "    )\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # 테이블 생성 예시\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS report_info (\n",
        "            pdf_file VARCHAR(100) PRIMARY KEY,\n",
        "            stock VARCHAR(50),\n",
        "            ticker VARCHAR(6) NOT NULL CHECK (LENGTH(ticker) = 6),\n",
        "            published_date DATE NOT NULL,\n",
        "            current_price INT,\n",
        "            target_price INT NOT NULL,\n",
        "            investment_opinion BOOLEAN,\n",
        "            author_analyst VARCHAR(15),\n",
        "            affiliated_firm VARCHAR(50)\n",
        "        );\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    print(\"REPORT_INFO 테이블이 성공적으로 생성되었습니다.\")\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS report_hit (\n",
        "            pdf_file VARCHAR(100) PRIMARY KEY,\n",
        "            hit_date DATE,\n",
        "            hit_days INT\n",
        "        );\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    print(\"REPORT_HIT 테이블이 성공적으로 생성되었습니다.\\n\")\n",
        "\n",
        "    # Assuming pdf_path, get_report_pdf_files, process_single_pdf, ask_gemini are defined in previous cells\n",
        "    # You might need to ensure these are defined or modify this part if they are not\n",
        "    try:\n",
        "      processed_results, conn, cursor = report_preprocessing_parallel_with_db(pdf_path, get_report_pdf_files, process_single_pdf, ask_gemini, conn=conn, cursor=cursor)\n",
        "    except NameError as e:\n",
        "      print(f\"Error: Required functions or variables are not defined. Please ensure all preceding cells are executed. Details: {e}\")\n",
        "\n",
        "\n",
        "    # 데이터 조회\n",
        "    cursor.execute(\"SELECT * FROM report_info\")\n",
        "    rows = cursor.fetchall()\n",
        "    print(\"\\n================ 데이터 조회 ================\", end=\"\")\n",
        "    print(\"\\n테이블 데이터:\")\n",
        "    for row in rows:\n",
        "        print(row)\n",
        "\n",
        "    cursor.execute(\"SELECT * FROM report_hit\")\n",
        "    rows = cursor.fetchall()\n",
        "    print(\"\\n테이블 데이터:\")\n",
        "    for row in rows:\n",
        "        print(row)\n",
        "\n",
        "    # 연결 종료\n",
        "    cursor.close()\n",
        "    conn.close()\n",
        "\n",
        "except (Exception, psycopg2.Error) as error:\n",
        "    print(f\"PostgreSQL 오류 발생: {error}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdlC_0DQh3y3",
        "outputId": "5ca2521c-9b9f-4262-df58-77806f09dcaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPORT_INFO 테이블이 성공적으로 생성되었습니다.\n",
            "REPORT_HIT 테이블이 성공적으로 생성되었습니다.\n",
            "\n",
            "Starting parallel processing for 5 files...\n",
            "Successfully processed and inserted data for: 210216_꾸준한_신작_출시와_흥행_스코어.pdf\n",
            "Successfully processed and inserted data for: 210216_실적보다는_파이프라인이_더_중요.pdf\n",
            "Successfully processed and inserted data for: 210216_4분기는_성수기_1분기는_최성수기.pdf\n",
            "Successfully processed and inserted data for: 210216_밸류에이션_부담_탈피_신작일정별_대응_유효.pdf\n",
            "Successfully processed and inserted data for: 210216_단기간의_업황_회복_기대보다는_추가_유동성_.pdf\n",
            "\n",
            "================ 데이터 조회 ================\n",
            "테이블 데이터:\n",
            "('210216_꾸준한_신작_출시와_흥행_스코어.pdf', '넷마블', '251270', datetime.date(2021, 2, 15), 138000, 170000, True, '김창권, 임희석', '미래에셋대우')\n",
            "('210216_실적보다는_파이프라인이_더_중요.pdf', 'SK디앤디', '210980', datetime.date(2021, 2, 16), 41650, 60000, True, '조윤호', 'DB금융투자')\n",
            "('210216_4분기는_성수기_1분기는_최성수기.pdf', '지역난방공사', '071320', datetime.date(2021, 2, 16), 38650, 60000, True, '유재선', '하나금융투자')\n",
            "('210216_밸류에이션_부담_탈피_신작일정별_대응_유효.pdf', '넷마블', '251270', datetime.date(2021, 2, 16), 134500, 142000, False, '성종화', '이베스트투자증권')\n",
            "('210216_단기간의_업황_회복_기대보다는_추가_유동성_.pdf', '제주항공', '089590', datetime.date(2021, 2, 16), 21950, 20000, False, '박성봉', '하나금융투자')\n",
            "\n",
            "테이블 데이터:\n",
            "('210216_꾸준한_신작_출시와_흥행_스코어.pdf', None, None)\n",
            "('210216_실적보다는_파이프라인이_더_중요.pdf', None, None)\n",
            "('210216_4분기는_성수기_1분기는_최성수기.pdf', datetime.date(2025, 5, 14), 1548)\n",
            "('210216_밸류에이션_부담_탈피_신작일정별_대응_유효.pdf', datetime.date(2021, 4, 16), 59)\n",
            "('210216_단기간의_업황_회복_기대보다는_추가_유동성_.pdf', datetime.date(2021, 3, 4), 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 중간평가\n",
        "\n",
        "* 테스트용 5개 정상 작동이지만 데이터 많아지면 429 에러 남\n",
        "\n",
        "  - 병렬 처리 안하면 에러 안 날지도...\n",
        "  - ollama 사용하면 자체 요청이니까 에러 안 날까...\n",
        "\n",
        "* report_hit 테이블 None, None은 목표 주가 달성 못함 (에러 아님)"
      ],
      "metadata": {
        "id": "WALgcG5b-DT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "|for pdf_file in ['210216_꾸준한_신작_출시와_흥행_스코어.pdf', '210216_실적보다는_파이프라인이_더_중요.pdf', '210216_4분기는_성수기_1분기는_최성수기.pdf',\n",
        "                 '210216_밸류에이션_부담_탈피_신작일정별_대응_유효.pdf', '210216_단기간의_업황_회복_기대보다는_추가_유동성_.pdf']:\n",
        "    print(\"Extracted Information (JSON) =>\", pdf_file)\n",
        "    print(ask_gemini(pdf_path, pdf_file))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBQFfeVa5lUu",
        "outputId": "fd94aac1-13e5-47b4-9c21-f00a6162dfb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Information (JSON) => 210216_꾸준한_신작_출시와_흥행_스코어.pdf\n",
            "{'종목명': '넷마블', '종목코드': '251270', '작성일': '2021-02-15', '현재 주가': 138000, '목표 주가': 170000, '투자 의견': 'Buy', '작성 애널리스트': '김창권, 임희석', '소속 증권사': '미래에셋대우'}\n",
            "Extracted Information (JSON) => 210216_실적보다는_파이프라인이_더_중요.pdf\n",
            "{'종목명': 'SK디앤디', '종목코드': '210980', '작성일': '2021-02-16', '현재 주가': 41650, '목표 주가': 60000, '투자 의견': 'Buy', '작성 애널리스트': '조윤호', '소속 증권사': 'DB금융투자'}\n",
            "Extracted Information (JSON) => 210216_4분기는_성수기_1분기는_최성수기.pdf\n",
            "{'종목명': '지역난방공사', '종목코드': '071320', '작성일': '2021-02-16', '현재 주가': 38650, '목표 주가': 60000, '투자 의견': 'BUY', '작성 애널리스트': '유재선', '소속 증권사': '하나금융투자'}\n",
            "Extracted Information (JSON) => 210216_밸류에이션_부담_탈피_신작일정별_대응_유효.pdf\n",
            "{'종목명': '넷마블', '종목코드': '251270', '작성일': '2021-02-16', '현재 주가': 134500, '목표 주가': 142000, '투자 의견': 'Hold', '작성 애널리스트': '성종화', '소속 증권사': '이베스트투자증권'}\n",
            "Extracted Information (JSON) => 210216_단기간의_업황_회복_기대보다는_추가_유동성_.pdf\n",
            "{'종목명': '제주항공', '종목코드': '089590', '작성일': '2021-02-16', '현재 주가': 21950, '목표 주가': 20000, '투자 의견': 'Hold', '작성 애널리스트': '박성봉', '소속 증권사': '하나금융투자'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pysaXFfC-3Zu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}