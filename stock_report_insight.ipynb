{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ASVedBBJHA4h",
        "outputId": "64b48100-7d9b-4d30-aa66-77f4e4ff4c24"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade google-ai-generativelanguage\n",
        "!pip install -q -U google-generativeai\n",
        "!pip install ollama pydantic pdfplumber langchain\n",
        "\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el1lwY2QG5zj"
      },
      "outputs": [],
      "source": [
        "# 실행 환경\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# 모델\n",
        "from google import genai\n",
        "import ollama\n",
        "from ollama import chat\n",
        "\n",
        "# RAG\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from pydantic import BaseModel\n",
        "import pdfplumber\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datetime import datetime\n",
        "import os, subprocess, time, json, re, shutil\n",
        "import warnings\n",
        "from typing import Callable\n",
        "\n",
        "# 병렬처리\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed#, ProcessPoolExecutor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivHXUegwIyUf"
      },
      "source": [
        "코랩 환경 여부"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD4jzrbMJwOA"
      },
      "outputs": [],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab  # 모듈이 있다면 Colab 환경\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "IS_COLAB_ENV = is_colab()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xOMpPqrd8WH"
      },
      "source": [
        "# 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQkrQqBWdKas"
      },
      "source": [
        "## PDF 임포트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52f69676",
        "outputId": "ed974de8-a024-48cc-f030-18b71bbce3ac"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive', force_remount=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4438d683",
        "outputId": "bcdf0a19-ddaa-434d-a8e8-9b2fffb0cfdb"
      },
      "outputs": [],
      "source": [
        "pdf_path = '/content/drive/My Drive/이어드림/pdfs'\n",
        "pdf_finished_path = '/content/drive/My Drive/이어드림/pdf_finished'\n",
        "\n",
        "if os.path.exists(pdf_path):\n",
        "    print(f\"디렉터리 '{pdf_path}'가 존재합니다.\")\n",
        "    # print(\"디렉터리 내용:\")\n",
        "    # for item in os.listdir(pdf_path):\n",
        "    #     print(item)\n",
        "else:\n",
        "    print(f\"디렉터리 '{pdf_path}'가 존재하지 않습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kyr7hs_3goCH"
      },
      "outputs": [],
      "source": [
        "# 테스트 5개만\n",
        "# 실제 실행 시는 이거 주석처리\n",
        "# pdf_path = '/content/drive/My Drive/이어드림/pdfs_test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cafe8589"
      },
      "outputs": [],
      "source": [
        "def get_report_pdf_files(directory_path, is_test:bool=False, test_num:int=5, verbose:bool=False) -> list:\n",
        "    all_items = os.listdir(directory_path)\n",
        "\n",
        "    # PDF 필터링\n",
        "    pdf_files = [item for item in all_items if item.endswith('.pdf')]\n",
        "\n",
        "    if is_test:\n",
        "        pdf_files = pdf_files[:test_num]\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Selected PDF files for processing:\")\n",
        "      for file in pdf_files:\n",
        "          print(file)\n",
        "\n",
        "    return pdf_files\n",
        "\n",
        "\n",
        "# selected_pdf_files = get_report_pdf_files(pdf_path, is_test=True, test_num=5)\n",
        "# selected_pdf_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J0PjH0Otn-u"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a highly skilled information extraction bot.\n",
        "Your task is to extract specific information from the provided securities report PDF file.\n",
        "Extract the following details and return them in JSON format:\n",
        "\n",
        "- 종목명 (Stock Name)\n",
        "- 종목코드 (티커) (Stock Code/Ticker)\n",
        "- 작성일 (Date of Report)\n",
        "- 현재 주가 (Current Stock Price - only numeric value)\n",
        "- 목표 주가 (Target Stock Price - only numeric value)\n",
        "- 투자 의견 (Investment Opinion - only in \"Buy\", \"Hold\" or \"Sell\")\n",
        "- 작성 애널리스트 (Author Analyst)\n",
        "- 소속 증권사 (Affiliated Securities Firm)\n",
        "\n",
        "If a piece of information is not found, use 'N/A' for string values and 0 for numeric values.\n",
        "\n",
        "Return only the JSON object. Do not include any other text.\n",
        "\n",
        "Example JSON format:\n",
        "{{\n",
        "  \"종목명\": \"Example Stock\",\n",
        "  \"종목코드\": \"000000\",\n",
        "  \"작성일\": \"YYYY-MM-DD\",\n",
        "  \"현재 주가\": 10000,\n",
        "  \"목표 주가\": 12000,\n",
        "  \"투자 의견\": \"BUY\",\n",
        "  \"작성 애널리스트\": \"Analyst Name\",\n",
        "  \"소속 증권사\": \"Securities Firm Name\"\n",
        "}}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KCl8QHQlkNd"
      },
      "source": [
        "## gemini api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE7zdR3gUszD"
      },
      "outputs": [],
      "source": [
        "def ask_gemini(directory_path:str, file_name:str, prompt:str=system_prompt, api_key:str=None, return_dict:bool=True, sleep:int=5) -> str:\n",
        "    if api_key is None:\n",
        "        if IS_COLAB_ENV:\n",
        "            api_key = userdata.get('GOOGLE_API_KEY') # Load the API key from Colab secrets\n",
        "        else:\n",
        "            load_dotenv()\n",
        "            api_key = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "    client = genai.Client(api_key=api_key)\n",
        "\n",
        "    if file_name is not None:\n",
        "        file_path = os.path.join(directory_path, file_name)\n",
        "\n",
        "        try:\n",
        "            # Upload the file using the genai client\n",
        "            sample_file = client.files.upload(file=file_path)\n",
        "\n",
        "            # Generate content using the uploaded file and the prompt\n",
        "            response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
        "                                                      contents=[sample_file, prompt])\n",
        "\n",
        "            result = response.text.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "\n",
        "            if sleep > 0:\n",
        "                time.sleep(sleep)\n",
        "\n",
        "            if return_dict:\n",
        "                return json.loads(result)\n",
        "            else:\n",
        "                return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "    else:\n",
        "        print(\"No PDF files were selected for processing. Please run the previous cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe1OrGXLlTN1"
      },
      "source": [
        "테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KGi7YByUlAW1"
      },
      "outputs": [],
      "source": [
        "# for pdf_file in selected_pdf_files:\n",
        "#     print(\"Extracted Information (JSON) =>\", pdf_file)\n",
        "#     print(ask_gemini(pdf_path, pdf_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwksNU95Gjbd"
      },
      "source": [
        "## ollama 로컬 (qwen, llama)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFNl-vML_ywM"
      },
      "source": [
        "**ollama 사용하지 않을 경우 실행 셀 주석처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7hgxe-4JOi_"
      },
      "source": [
        "ollama 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5355axNOpn2g"
      },
      "outputs": [],
      "source": [
        "# subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "# time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nn11NyLJQ4O"
      },
      "source": [
        "### 로컬 모델 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnRayeXVpwsv"
      },
      "outputs": [],
      "source": [
        "def ollama_model_check(drive_model_dir:str=None, require_lite:bool=True) -> str:\n",
        "    drive_model_dir = \"/content/drive/MyDrive/ollama_models\" if drive_model_dir is None else drive_model_dir\n",
        "    local_model_dir = os.path.expanduser(\"~/.ollama\")\n",
        "\n",
        "    # 모델 저장 폴더 설정\n",
        "    os.makedirs(drive_model_dir, exist_ok=True)\n",
        "    # os.makedirs(os.path.expanduser(\"~/.ollama\"), exist_ok=True)\n",
        "\n",
        "    if os.path.exists(local_model_dir):\n",
        "        os.system(f\"rm -rf {local_model_dir}\")\n",
        "    os.system(f\"ln -s {drive_model_dir} {local_model_dir}\")\n",
        "\n",
        "    model_names = [\"qwen3:8b\", \"llama3:8b\"] if not require_lite else [\"qwen3:1.7b\", \"llama3:8b\"]\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True, check=True)\n",
        "        installed_models_output = result.stdout\n",
        "    except subprocess.CalledProcessError:\n",
        "        installed_models_output = \"\"\n",
        "\n",
        "    for model in model_names:\n",
        "        if model in installed_models_output:\n",
        "            print(f\"{model} 이미 설치됨 — 다운로드 생략\")\n",
        "        else:\n",
        "            print(f\"{model} 다운로드 중...\")\n",
        "            subprocess.run([\"ollama\", \"pull\", model], check=True)\n",
        "\n",
        "    print(\"\\n모델 확인\")\n",
        "\n",
        "    return drive_model_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCS1fr_spykw"
      },
      "outputs": [],
      "source": [
        "# # %time\n",
        "# model_path = ollama_model_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsLfi-PeJXOu"
      },
      "source": [
        "로컬 환경 ollama 모델 리스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YBkiChEp1G4"
      },
      "outputs": [],
      "source": [
        "# !ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esVlug6hJb72"
      },
      "source": [
        "### 모델 출력 포맷"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgVXkQGGadBP"
      },
      "outputs": [],
      "source": [
        "class ReportInfo(BaseModel):\n",
        "    stock: str\n",
        "    ticker: str\n",
        "    published_date: str\n",
        "    current_price: float\n",
        "    target_price: float\n",
        "    investment_opinion: bool\n",
        "    author_analyst: str\n",
        "    affiliated_firm: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTYE3M4IrNyN"
      },
      "outputs": [],
      "source": [
        "ollama_prompts = {\"종목명\": \"본 보고서가 취급하는 종목명이 무엇인가요?\",\n",
        "                  \"종목코드\": \"본 보고서가 취급하는 기업의 티커(ticker)가 무엇인가요?\",\n",
        "                  \"작성일\": \"본 보고서가 발행된 일시를 yyyy-mm-dd 형태로 답하세요.\",\n",
        "                  \"현재 주가\": \"본 보고서에 발표된 현재 주가를 답하세요. (KRW)\",\n",
        "                  \"목표 주가\": \"본 보고서에 발표된 목표 주가를 답하세요. (KRW)\",\n",
        "                  \"투자 의견\": \"본 보고서에 발표된 투자 의견을 답하세요. (True: 매수, False: 보유)\",\n",
        "                  \"작성 애널리스트\": \"본 보고서에 발표된 작성 애널리스트는 누구인가요?\",\n",
        "                  \"소속 증권사\": \"본 보고서에 발표된 소속 증권사의 기업명은 무엇인가요?\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDJ2r5qJJx2W"
      },
      "source": [
        "### 문서 참조"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPwRwFObJ31m"
      },
      "source": [
        "PDF에서 표, 그래프 제외한 텍스트만 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJfIgWdFhTNu"
      },
      "outputs": [],
      "source": [
        "def pdf_to_text(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# def pdf_to_text_no_tables(pdf_path):\n",
        "#     text = \"\"\n",
        "#     with pdfplumber.open(pdf_path) as pdf:\n",
        "#         for page in pdf.pages:\n",
        "#             # 페이지의 표 영역 추출\n",
        "#             tables = page.find_tables()\n",
        "#             table_bboxes = [table.bbox for table in tables]  # bbox = (x0, top, x1, bottom)\n",
        "\n",
        "#             # 문자 단위로 필터링\n",
        "#             page_text = \"\"\n",
        "#             for char in page.chars:\n",
        "#                 in_table = any(\n",
        "#                     bbox[0] <= char['x0'] <= bbox[2] and\n",
        "#                     bbox[1] <= char['top'] <= bbox[3]\n",
        "#                     for bbox in table_bboxes\n",
        "#                 )\n",
        "#                 if not in_table:\n",
        "#                     page_text += char['text']\n",
        "#             text += page_text + \"\\n\"\n",
        "#     return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hIOIumpJ8OO"
      },
      "source": [
        "코사인 유사도 topk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4WWTxtvhjlG"
      },
      "outputs": [],
      "source": [
        "def get_simliar_chunks(store, query_vector, chunks:list, k:int=2):\n",
        "    # Convert the store (list of vectors) to a numpy array\n",
        "    store_np = np.array(store)\n",
        "    query_vector_np = np.array(query_vector)\n",
        "\n",
        "    # Calculate cosine similarity between the query vector and each chunk vector\n",
        "    dot_products = np.dot(store_np, query_vector_np)\n",
        "    norm_store = np.linalg.norm(store_np, axis=1)\n",
        "    norm_query = np.linalg.norm(query_vector_np)\n",
        "\n",
        "    # Avoid division by zero for zero vectors\n",
        "    norm_store[norm_store == 0] = 1e-10\n",
        "    norm_query = norm_query if norm_query != 0 else 1e-10\n",
        "\n",
        "    cosine_similarities = dot_products / (norm_store * norm_query)\n",
        "\n",
        "    # Get the indices that would sort the cosine similarities in descending order\n",
        "    sorted_indices = np.argsort(cosine_similarities)[-1:-k-1:-1]\n",
        "\n",
        "    # Get the sorted chunks and similarity scores\n",
        "    sorted_chunks = [chunks[i] for i in sorted_indices]\n",
        "    sorted_scores = cosine_similarities[sorted_indices]\n",
        "\n",
        "    # Return the sorted chunks and similarity scores\n",
        "    return sorted_chunks, sorted_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml4rOI9uhoDP"
      },
      "outputs": [],
      "source": [
        "def ask_ollama(directory_path:str, file_name:str, model_name:str, prompt:str=system_prompt, format:BaseModel=ReportInfo, return_dict:bool=True):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "\n",
        "    file_path = os.path.join(directory_path, file_name)\n",
        "    text = pdf_to_text(file_path)\n",
        "    chunks = text_splitter.split_text(text)\n",
        "\n",
        "    store_emb = ollama.embed(model=model_name, input=chunks)['embeddings'] # Extract embeddings from the EmbedResponse objects\n",
        "    topk_chunks = []\n",
        "\n",
        "    for question in ollama_prompts.values():\n",
        "        query_v_emb = ollama.embed(model=model_name, input=question)['embeddings'][0]\n",
        "        rel_chunks, _ = get_simliar_chunks(store_emb, query_v_emb, chunks)\n",
        "        topk_chunks.extend(rel_chunks)\n",
        "        # print(question, \"\\n==>\", rel_chunks)\n",
        "\n",
        "    response = chat(model=model_name,\n",
        "                    messages=[{'role': 'system', 'content': prompt},\n",
        "                              {'role': 'user', 'content': 'Give me the context!'},\n",
        "                              {'role': 'assistant', 'content': '\\n'.join(topk_chunks)},\n",
        "                              {'role': 'user', 'content': 'Return a JSON object from the reference as per my instructions.'}],\n",
        "                    format=ReportInfo.model_json_schema(),\n",
        "                    # think=True,\n",
        "                    stream=False)\n",
        "\n",
        "    result = ReportInfo.model_validate_json(response.message.content)\n",
        "\n",
        "    if return_dict:\n",
        "        return result.model_dump()\n",
        "    else:\n",
        "        return result.model_dump_json(indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtRAxanf_mhN"
      },
      "source": [
        "테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tt_c-zGgvgc"
      },
      "outputs": [],
      "source": [
        "# ask_ollama('/content/drive/My Drive/이어드림/pdfs', \"210216_4분기는_성수기_1분기는_최성수기.pdf\", \"llama3:8b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs7YHPrrjL67"
      },
      "outputs": [],
      "source": [
        "# for pdf_file in ['210226_이제_조정은_충분하다.pdf', '210226_플랫폼_기반의_간편건강식_전문기업_중국_시.pdf', '210226_재무_건전성_및_투자여력_확보.pdf',\n",
        "#                   '210302_DRAM_가격_전망치_상향_조정.pdf', '210226_소방설비_제작부터_소방시설_공사까지_종합_.pdf']:\n",
        "#     print(\"Extracted Information (JSON) =>\", pdf_file)\n",
        "#     print(ask_ollama(pdf_path, pdf_file, \"llama3:8b\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTqlylfvd6cA"
      },
      "source": [
        "## 목표 주가 수집"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0DYGrhMteQa2",
        "outputId": "01dd8c9f-ac05-4bca-ece5-4c37d39d6270"
      },
      "outputs": [],
      "source": [
        "!pip install pykrx\n",
        "\n",
        "from pykrx import stock\n",
        "import psycopg2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5rC_PPhtNxl"
      },
      "outputs": [],
      "source": [
        "def find_target_hit_date(ticker:str, report_date:str, target_price:float):\n",
        "    start_date = report_date.replace(\"-\", \"\")\n",
        "    end_date = datetime.today().strftime(\"%Y%m%d\")\n",
        "\n",
        "    df = stock.get_market_ohlcv_by_date(start_date, end_date, ticker)\n",
        "    df = df[[\"종가\"]]\n",
        "\n",
        "    reached = df[df[\"종가\"] >= target_price]\n",
        "\n",
        "    if not reached.empty:\n",
        "        first_hit_date = reached.index[0].strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        report_dt = datetime.strptime(report_date, \"%Y-%m-%d\")\n",
        "        hit_dt = datetime.strptime(first_hit_date, \"%Y-%m-%d\")\n",
        "\n",
        "        return first_hit_date, (hit_dt - report_dt).days\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ2UGCUy7tWd"
      },
      "source": [
        "## PDF 전처리 파이프라인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnEkltZ2hK9d"
      },
      "source": [
        "PDF로부터 LLM이 추출한 데이터 중 필수 데이터 검증"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJTbzoq4W_WY"
      },
      "outputs": [],
      "source": [
        "def is_validate_report_data(report_data:dict) -> bool:\n",
        "    \"\"\"\n",
        "    종목코드, 작성일, 목표 주가 => 필수\n",
        "    종목명, 현재 주가, 투자 의견, 작성 애널리스트, 소속 증권사 => 선택\n",
        "    \"\"\"\n",
        "    na_val = [None, \"N/A\", \"n/a\", \"\", 0]\n",
        "    return report_data \\\n",
        "          and isinstance(report_data, dict) \\\n",
        "          and report_data.get(\"종목코드\") not in na_val \\\n",
        "          and report_data.get(\"작성일\") not in na_val \\\n",
        "          and report_data.get(\"목표 주가\") not in na_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAsC7YTV705r"
      },
      "source": [
        "단일 PDF 파일"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd8d55ed"
      },
      "outputs": [],
      "source": [
        "def process_single_pdf(file_name:str, directory_path:str, feat_extractor:Callable, target_hitter:Callable, feat_extractor_kwargs:dict=None) -> dict:\n",
        "    \"\"\"\n",
        "    Processes a single PDF file by extracting info and finding target hit date.\n",
        "    \"\"\"\n",
        "    report_info = None\n",
        "    hit_date_info = None\n",
        "\n",
        "    try:\n",
        "        feat_extractor_kwargs = {} if feat_extractor_kwargs is None else feat_extractor_kwargs\n",
        "        # Task 1: Extract information using ask_gemini (I/O-bound)\n",
        "        report_info = feat_extractor(directory_path, file_name, **feat_extractor_kwargs)\n",
        "\n",
        "        # Check if essential info is available for target hit date calculation\n",
        "        if is_validate_report_data(report_info):\n",
        "            ticker = report_info[\"종목코드\"]\n",
        "            report_date = report_info[\"작성일\"]\n",
        "            target_price = report_info[\"목표 주가\"]\n",
        "\n",
        "            try:\n",
        "                # Task 2: Find target hit date (potentially CPU-bound, but often quick with PyKRX)\n",
        "                hit_date_info = target_hitter(ticker, report_date, target_price)\n",
        "            except Exception as hit_error:\n",
        "                hit_date_info = f\"Error finding target hit date: {hit_error}\"\n",
        "\n",
        "            # Combine the results\n",
        "            return {\n",
        "                \"pdf_file\": file_name,\n",
        "                \"report_info\": report_info, # Return report_info even if hit_date_info failed\n",
        "                \"hit_date_info\": hit_date_info\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"pdf_file\": file_name,\n",
        "                \"report_info\": report_info, # Return potentially incomplete report_info\n",
        "                \"hit_date_info\": \"Could not extract essential information for target hit date.\"\n",
        "            }\n",
        "\n",
        "    except Exception as extract_error:\n",
        "        print(f\"Error processing {file_name} during feature extraction: {extract_error}\")\n",
        "        return {\n",
        "            \"pdf_file\": file_name,\n",
        "            \"report_info\": None, # Return None if feature extraction failed\n",
        "            \"hit_date_info\": f\"Error during feature extraction: {extract_error}\"\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzK1J30Q72R8"
      },
      "source": [
        "파이프라인 (DB 작업 분리용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC1s-A7D3MtN"
      },
      "outputs": [],
      "source": [
        "def report_preprocessing_parallel(directory_path:str, get_files_fn:Callable, pipeline_fn:Callable, feat_extractor:Callable, target_hitter:Callable, feat_extractor_kwargs:dict=None,\n",
        "                                  num_workers:int=5, verbose:bool=True) -> list:\n",
        "    \"\"\"\n",
        "    Parallel Processing Implementation for Report Preprocessing.\n",
        "    \"\"\"\n",
        "    processed_results = []\n",
        "\n",
        "    # Using ThreadPoolExecutor for parallel execution\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        # Get the list of PDF files using the executor\n",
        "        future_to_get_files = executor.submit(get_files_fn, directory_path)\n",
        "        selected_pdf_files = future_to_get_files.result() # Wait for the file list to be ready\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Starting parallel processing for {len(selected_pdf_files)} files...\")\n",
        "\n",
        "        # Submit tasks for processing each PDF file\n",
        "        future_to_pdf = {executor.submit(pipeline_fn, pdf_file, directory_path, feat_extractor, target_hitter, feat_extractor_kwargs): pdf_file for pdf_file in selected_pdf_files}\n",
        "\n",
        "        # Process the results as they complete\n",
        "        for future in as_completed(future_to_pdf):\n",
        "            pdf_file = future_to_pdf[future]\n",
        "            try:\n",
        "                result = future.result() # result like dict {\"pdf_file\": str, \"report_info\": json, \"hit_date_info\": tuple}\n",
        "                processed_results.append(result)\n",
        "                if verbose:\n",
        "                    print(f\"Successfully processed data for: {pdf_file}\")\n",
        "            except Exception as exc:\n",
        "                print(f'{pdf_file} generated an exception during processing: {exc}')\n",
        "\n",
        "    return processed_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zG5Ou6r8J_8"
      },
      "outputs": [],
      "source": [
        "# # 다중 LLM 함수 사용 시에도 파라미터 등 달라서 아래 함수 다중 호출로 처리\n",
        "# processed_results = report_preprocessing_parallel(pdf_path, get_report_pdf_files, process_single_pdf, ask_gemini)\n",
        "\n",
        "# # --- Displaying Results ---\n",
        "# print(\"\\n--- Summary of Processed Results ---\")\n",
        "# for result in processed_results:\n",
        "#     print(f\"File: {result['pdf_file']}\")\n",
        "#     print(f\"  Report Info: {result['report_info']}\")\n",
        "#     print(f\"  Target Hit Info: {result['hit_date_info']}\")\n",
        "#     print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmhrmGAvfO6B"
      },
      "source": [
        "파이프라인 (DB 작업 통합용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6bf30e2"
      },
      "outputs": [],
      "source": [
        "def report_preprocessing_parallel_with_db(directory_path:str, get_files_fn:Callable, pipeline_fn:Callable, feat_extractor:Callable, target_hitter:Callable, feat_extractor_kwargs:dict=None,\n",
        "                                          num_workers:int=5, verbose:bool=True, conn=None, cursor=None) -> tuple:\n",
        "    \"\"\"\n",
        "    Parallel Processing Implementation for Report Preprocessing with direct DB insertion.\n",
        "    Assumes DB connection 'conn' and cursor 'cursor' are available in the scope where this function is called.\n",
        "    \"\"\"\n",
        "    if conn is None or cursor is None:\n",
        "        raise ValueError(\"DB connection and cursor must be provided.\")\n",
        "\n",
        "    processed_results = []\n",
        "\n",
        "    # Using ThreadPoolExecutor for parallel execution\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        # Get the list of PDF files using the executor\n",
        "        future_to_get_files = executor.submit(get_files_fn, directory_path)\n",
        "        selected_pdf_files = future_to_get_files.result() # Wait for the file list to be ready\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Starting parallel processing for {len(selected_pdf_files)} files...\")\n",
        "\n",
        "        # Submit tasks for processing each PDF file\n",
        "        future_to_pdf = {executor.submit(pipeline_fn, pdf_file, directory_path, feat_extractor, target_hitter, feat_extractor_kwargs): pdf_file for pdf_file in selected_pdf_files}\n",
        "\n",
        "        # Process the results as they complete\n",
        "        for future in as_completed(future_to_pdf):\n",
        "            pdf_file = future_to_pdf[future]\n",
        "            try:\n",
        "                result = future.result() # result like dict {\"pdf_file\": str, \"report_info\": json, \"hit_date_info\": tuple}\n",
        "                processed_results.append(result)\n",
        "\n",
        "                # --- Direct DB Insertion within the loop ---\n",
        "                report_info = result.get(\"report_info\")\n",
        "                hit_date_info = result.get(\"hit_date_info\") # tuple or None (normal) / str (abnormal)\n",
        "                file_name = result.get(\"pdf_file\")\n",
        "\n",
        "                # Check if essential information was extracted successfully before attempting insert\n",
        "                if is_validate_report_data(report_info):\n",
        "                    # Prepare data for report_info table\n",
        "                    # If a constraint error occurs, judge as a data error and passed\n",
        "                    report_info_data = (\n",
        "                        file_name,\n",
        "                        report_info.get(\"종목명\"),\n",
        "                        report_info.get(\"종목코드\"),\n",
        "                        report_info.get(\"작성일\"),\n",
        "                        report_info.get(\"현재 주가\"),\n",
        "                        report_info.get(\"목표 주가\"),\n",
        "                        report_info.get(\"투자 의견\").lower() == \"buy\",\n",
        "                        report_info.get(\"작성 애널리스트\"),\n",
        "                        report_info.get(\"소속 증권사\")\n",
        "                    )\n",
        "\n",
        "                    try:\n",
        "                        cursor.execute(\"\"\"\n",
        "                            INSERT INTO report_info (pdf_file, stock, ticker, published_date, current_price, target_price, investment_opinion, author_analyst, affiliated_firm)\n",
        "                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);\n",
        "                        \"\"\", report_info_data)\n",
        "\n",
        "                        conn.commit()\n",
        "                        if verbose:\n",
        "                            print(f\"Successfully extracted data for: {pdf_file}\")\n",
        "                            try:\n",
        "                                shutil.move(os.path.join(pdf_path, pdf_file), os.path.join(pdf_finished_path, pdf_file))\n",
        "                                print(f\"Moved {pdf_file} with completely extracted data to: {pdf_finished_path}/\")\n",
        "                            except FileNotFoundError:\n",
        "                                print(f\"{pdf_file} not found in {pdf_path}/\")\n",
        "                            except PermissionError:\n",
        "                                print(f\"No permission to move file: {pdf_file}\")\n",
        "                            except Exception as shutil_exc:\n",
        "                                print(f\"Failed to move file {pdf_file}: {shutil_exc}\")\n",
        "\n",
        "                    except (Exception, psycopg2.Error) as db_error_info:\n",
        "                        conn.rollback()\n",
        "                        print(f\"REPORT_INFO table INSERT error for {pdf_file}: {db_error_info}\")\n",
        "\n",
        "                    if isinstance(hit_date_info, str) and hit_date_info.startswith(\"Error finding target hit date:\"):\n",
        "                        # If target_hitter occurs error alone, prevent insert None into REPORT_HIT.\n",
        "                        # None in REPORT_HIT table means Hit miss, not error.\n",
        "                        print(f\"Error occurs only on target_hitter. REPORT_HIT table INSERT is passed for {pdf_file}: {hit_date_info}\")\n",
        "                    else:\n",
        "                        # Prepare data for report_hit table\n",
        "                        hit_date = hit_date_info[0] if isinstance(hit_date_info, tuple) else None\n",
        "                        hit_days = hit_date_info[1] if isinstance(hit_date_info, tuple) else None\n",
        "\n",
        "                        # If both hit_date and hit_days are None, judge as a Hit miss\n",
        "                        report_hit_data = (\n",
        "                            file_name,\n",
        "                            hit_date,\n",
        "                            hit_days\n",
        "                        )\n",
        "\n",
        "                        try:\n",
        "                            cursor.execute(\"\"\"\n",
        "                                INSERT INTO report_hit (pdf_file, hit_date, hit_days)\n",
        "                                VALUES (%s, %s, %s);\n",
        "                            \"\"\", report_hit_data)\n",
        "\n",
        "                            conn.commit()\n",
        "                            if verbose:\n",
        "                                print(f\"Successfully processed and inserted data for: {pdf_file}\")\n",
        "\n",
        "                        except (Exception, psycopg2.Error) as db_error_hit:\n",
        "                            conn.rollback()\n",
        "                            print(f\"REPORT_HIT table INSERT error for {pdf_file}: {db_error_hit}\")\n",
        "                            # Log this error or handle it as needed without stopping the loop\n",
        "\n",
        "                else:\n",
        "                    if verbose:\n",
        "                         print(f\"Skipping DB insert for {pdf_file}: Essential info missing or processing error.\")\n",
        "\n",
        "            except Exception as exc:\n",
        "                print(f'{pdf_file} generated an exception during processing: {exc}')\n",
        "                # This catches errors during the PDF processing pipeline_fn\n",
        "\n",
        "    return processed_results, conn, cursor # You might still want to return results for logging or further processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXB0IPZUh5ru"
      },
      "source": [
        "## DB 연결 (localhost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6B6h99fjKhb"
      },
      "outputs": [],
      "source": [
        "if IS_COLAB_ENV:\n",
        "    postgres_key = userdata.get('POSTGRES_KEY')\n",
        "else:\n",
        "    load_dotenv()\n",
        "    postgres_key = os.getenv('POSTGRES_KEY')\n",
        "\n",
        "db_name = \"stockdb\"\n",
        "db_user = \"stock\"\n",
        "\n",
        "user_config_query = f\"CREATE USER {db_user} WITH PASSWORD '{postgres_key}';\"\n",
        "db_config_query = f\"CREATE DATABASE {db_name} OWNER {db_user};\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkkYvgBChY-5",
        "outputId": "d49ff976-d1be-4de1-95fb-fa4a1f8562ca"
      },
      "outputs": [],
      "source": [
        "# !sudo apt-get update\n",
        "!sudo apt-get install -y postgresql\n",
        "\n",
        "!sudo service postgresql start\n",
        "\n",
        "# Drop user and database if they exist to ensure a clean setup\n",
        "!sudo -u postgres psql -c \"DROP DATABASE IF EXISTS {db_name};\"\n",
        "!sudo -u postgres psql -c \"DROP USER IF EXISTS {db_user};\"\n",
        "\n",
        "# Create the user with password and login privileges\n",
        "!sudo -u postgres psql -c \"CREATE USER {db_user} WITH PASSWORD '{postgres_key}' LOGIN;\"\n",
        "\n",
        "# Create the database owned by the user\n",
        "!sudo -u postgres psql -c \"CREATE DATABASE {db_name} OWNER {db_user};\"\n",
        "\n",
        "!pip install psycopg2-binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdlC_0DQh3y3",
        "outputId": "5f945abb-64b1-4865-d4a5-d11102afcc97"
      },
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "\n",
        "# 연결 정보 설정\n",
        "try:\n",
        "    # 데이터베이스 연결\n",
        "    conn = psycopg2.connect(\n",
        "        host=\"localhost\",\n",
        "        dbname=db_name,\n",
        "        user=db_user,\n",
        "        password=postgres_key\n",
        "    )\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # 테이블 생성\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS report_info (\n",
        "            pdf_file VARCHAR(100) PRIMARY KEY,\n",
        "            stock VARCHAR(50),\n",
        "            ticker VARCHAR(6) NOT NULL CHECK (LENGTH(ticker) = 6),\n",
        "            published_date DATE NOT NULL,\n",
        "            current_price INT,\n",
        "            target_price INT NOT NULL,\n",
        "            investment_opinion BOOLEAN,\n",
        "            author_analyst VARCHAR(15),\n",
        "            affiliated_firm VARCHAR(50)\n",
        "        );\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    print(\"REPORT_INFO 테이블이 성공적으로 생성되었습니다.\")\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS report_hit (\n",
        "            pdf_file VARCHAR(100) PRIMARY KEY,\n",
        "            hit_date DATE,\n",
        "            hit_days INT\n",
        "        );\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    print(\"REPORT_HIT 테이블이 성공적으로 생성되었습니다.\\n\")\n",
        "\n",
        "    # Assuming pdf_path, get_report_pdf_files, process_single_pdf, ask_gemini are defined in previous cells\n",
        "    # You might need to ensure these are defined or modify this part if they are not\n",
        "    try:\n",
        "      processed_results, conn, cursor = report_preprocessing_parallel_with_db(pdf_path,\n",
        "                                                                              get_report_pdf_files,\n",
        "                                                                              process_single_pdf,\n",
        "                                                                              ask_gemini,\n",
        "                                                                              find_target_hit_date,\n",
        "                                                                              num_workers=1,\n",
        "                                                                              conn=conn, cursor=cursor)\n",
        "    except NameError as e:\n",
        "      print(f\"Error: Required functions or variables are not defined. Please ensure all preceding cells are executed. Details: {e}\")\n",
        "\n",
        "\n",
        "    # 데이터 조회\n",
        "    cursor.execute(\"SELECT * FROM report_info;\")\n",
        "    rows = cursor.fetchall()\n",
        "    print(\"\\n================ 데이터 조회 ================\", end=\"\")\n",
        "    print(\"\\n테이블 데이터:\")\n",
        "    for row in rows:\n",
        "        print(row)\n",
        "\n",
        "    cursor.execute(\"SELECT * FROM report_hit;\")\n",
        "    rows = cursor.fetchall()\n",
        "    print(\"\\n테이블 데이터:\")\n",
        "    for row in rows:\n",
        "        print(row)\n",
        "\n",
        "    # 연결 종료\n",
        "    cursor.close()\n",
        "    conn.close()\n",
        "\n",
        "except (Exception, psycopg2.Error) as error:\n",
        "    print(f\"PostgreSQL 오류 발생: {error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WALgcG5b-DT_"
      },
      "source": [
        "# 중간평가\n",
        "\n",
        "* 테스트용 5개 정상 작동이지만 데이터 많아지면 429 에러 남\n",
        "\n",
        "  - 병렬 처리 안하면 에러 안 날지도...\n",
        "  - ollama 사용하면 자체 요청이니까 에러 안 날까...\n",
        "\n",
        "* report_hit 테이블 None, None은 목표 주가 달성 못함 (에러 아님)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llqXKRNVoaUb"
      },
      "source": [
        "막대그래프,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pysaXFfC-3Zu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
